{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HallwayType:  ['terraced' 'corridor' 'mixed']\n",
      "HeatingType ['individual_heating' 'central_heating']\n",
      "AptManageType ['management_in_trust' 'self_management']\n",
      "TimeToBusStop ['5min~10min' '0~5min' '10min~15min']\n",
      "TimeToSubway ['10min~15min' '5min~10min' '0-5min' '15min~20min' 'no_bus_stop_nearby']\n",
      "SubwayStation ['Kyungbuk_uni_hospital' 'Daegu' 'Sin-nam' 'Myung-duk' 'Chil-sung-market'\n",
      " 'Bangoge' 'Banwoldang' 'no_subway_nearby']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['terraced', 'corridor', 'mixed'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "print('HallwayType: ', train_data['HallwayType'].unique())\n",
    "print('HeatingType', train_data['HeatingType'].unique())\n",
    "print('AptManageType', train_data['AptManageType'].unique())\n",
    "print('TimeToBusStop', train_data['TimeToBusStop'].unique())\n",
    "print('TimeToSubway', train_data['TimeToSubway'].unique())\n",
    "print('SubwayStation', train_data['SubwayStation'].unique())\n",
    "\n",
    "df = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Separate the target variable\n",
    "# target = df['SalePrice']\n",
    "# df = df.drop('SalePrice', axis=1)\n",
    "\n",
    "# # Scale the target variable\n",
    "# target_scaler = MinMaxScaler()\n",
    "# target_scaled = target_scaler.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "# df['SalePrice_scaled'] = target_scaled\n",
    "\n",
    "# # One-hot encode the SubwayStation column\n",
    "# df = pd.get_dummies(df, columns=['SubwayStation'])\n",
    "# df = df.drop('SubwayStation_no_subway_nearby', axis=1)\n",
    "\n",
    "# # Map the TimeToSubway column to numerical values\n",
    "# time_to_subway_map = {'no_bus_stop_nearby': 0, '0-5min': 1, '5min~10min': 0.8, '10min~15min': 0.6, '15min~20min': 0.4}\n",
    "# df['TimeToSubway'] = df['TimeToSubway'].map(time_to_subway_map)\n",
    "\n",
    "# # Map the HallwayType column to numerical values\n",
    "# hallway_type_map = {'terraced': 0, 'mixed': 1, 'corridor': 2}\n",
    "# df['HallwayType'] = df['HallwayType'].map(hallway_type_map)\n",
    "\n",
    "# # Mapping for HeatingType column\n",
    "# heating_type_mapping = {'individual_heating': 0, 'central_heating': 1}\n",
    "# df['HeatingType'] = df['HeatingType'].map(heating_type_mapping)\n",
    "\n",
    "# # Mapping for AptManageType column\n",
    "# apt_manage_type_mapping = {'management_in_trust': 0, 'self_management': 1}\n",
    "# df['AptManageType'] = df['AptManageType'].map(apt_manage_type_mapping)\n",
    "\n",
    "# # Mapping for TimeToBusStop column\n",
    "# time_to_bus_stop_mapping = {'0~5min': 0, '5min~10min': 0.5, '10min~15min': 1.0}\n",
    "# df['TimeToBusStop'] = df['TimeToBusStop'].map(time_to_bus_stop_mapping)\n",
    "\n",
    "# # Scale the numerical features using MinMaxScaler\n",
    "# num_cols = ['YearBuilt', 'Size(sqf)', 'Floor', 'HallwayType', 'HeatingType', 'AptManageType', 'N_Parkinglot(Ground)', 'N_Parkinglot(Basement)', 'TimeToBusStop', 'N_manager', 'N_elevators', 'N_FacilitiesInApt', 'N_FacilitiesNearBy(Total)', 'N_SchoolNearBy(Total)']\n",
    "# scaler = MinMaxScaler()\n",
    "# df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "# # Combine the preprocessed data with the target variable\n",
    "# df = pd.concat([target, df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def get_data_loaders(hallway_type_onehot=False, batch_size=32, val_split=0.2, subway_cost=0.4):\n",
    "    df_train = pd.read_csv('train_data.csv')\n",
    "    df_test = pd.read_csv('test_data.csv')\n",
    "\n",
    "    # Separate the target variable\n",
    "    target = df_train['SalePrice']\n",
    "    df_train = df_train.drop('SalePrice', axis=1)\n",
    "\n",
    "    # Scale the target variable\n",
    "    target_scaler = MinMaxScaler()\n",
    "    target_scaled = target_scaler.fit_transform(target.values.reshape(-1, 1))\n",
    "    df_train['SalePrice_scaled'] = target_scaled\n",
    "\n",
    "    # One-hot encode the SubwayStation column\n",
    "    df_train = pd.get_dummies(df_train, columns=['SubwayStation'])\n",
    "    df_train = df_train.drop('SubwayStation_no_subway_nearby', axis=1)\n",
    "\n",
    "    # Map the TimeToSubway column to numerical values\n",
    "    sub = (1-subway_cost)/3\n",
    "    subway_vals = [0, subway_cost, subway_cost + sub, subway_cost + 2*sub, 1]\n",
    "    sabway_keys = ['no_bus_stop_nearby', '15min~20min', '10min~15min', '5min~10min', '0-5min']\n",
    "    time_to_subway_map = dict(zip(sabway_keys, subway_vals))\n",
    "    df_train['TimeToSubway'] = df_train['TimeToSubway'].map(time_to_subway_map)\n",
    "\n",
    "    # Map the HallwayType column to numerical values\n",
    "    if hallway_type_onehot:\n",
    "        df_train = pd.get_dummies(df_train, columns=['HallwayType'])\n",
    "    else:\n",
    "        hallway_type_map = {'terraced': 0, 'mixed': 1, 'corridor': 2}\n",
    "        df_train['HallwayType'] = df_train['HallwayType'].map(hallway_type_map)\n",
    "    \n",
    "    # Mapping for HeatingType column\n",
    "    heating_type_mapping = {'individual_heating': 0, 'central_heating': 1}\n",
    "    df_train['HeatingType'] = df_train['HeatingType'].map(heating_type_mapping)\n",
    "\n",
    "    # Mapping for AptManageType column\n",
    "    apt_manage_type_mapping = {'management_in_trust': 0, 'self_management': 1}\n",
    "    df_train['AptManageType'] = df_train['AptManageType'].map(apt_manage_type_mapping)\n",
    "\n",
    "    # Mapping for TimeToBusStop column\n",
    "    time_to_bus_stop_mapping = {'0~5min': 0, '5min~10min': 0.5, '10min~15min': 1.0}\n",
    "    df_train['TimeToBusStop'] = df_train['TimeToBusStop'].map(time_to_bus_stop_mapping)\n",
    "\n",
    "    # Scale the numerical features using MinMaxScaler\n",
    "    num_cols = ['YearBuilt', 'Size(sqf)', 'Floor', 'HeatingType', 'AptManageType', 'N_Parkinglot(Ground)', 'N_Parkinglot(Basement)', 'N_manager', 'N_elevators', 'N_FacilitiesInApt', 'N_FacilitiesNearBy(Total)', 'N_SchoolNearBy(Total)']\n",
    "    if not hallway_type_onehot:\n",
    "        num_cols.append('HallwayType')\n",
    "    scaler = MinMaxScaler()\n",
    "    df_train[num_cols] = scaler.fit_transform(df_train[num_cols])\n",
    "\n",
    "    # Combine the preprocessed data with the target variable\n",
    "    df_train = pd.concat([target, df_train], axis=1)\n",
    "\n",
    "    # Split the data into train and validation sets\n",
    "    train_size = int((1-val_split) * len(df_train))\n",
    "    df_train, df_val = df_train[:train_size], df_train[train_size:]\n",
    "\n",
    "    # Separate the target variable from the train and validation sets\n",
    "    target_train = df_train['SalePrice_scaled']\n",
    "    df_train = df_train.drop(['SalePrice', 'SalePrice_scaled'], axis=1)\n",
    "\n",
    "    target_val = df_val['SalePrice']\n",
    "    df_val = df_val.drop(['SalePrice', 'SalePrice_scaled'], axis=1)\n",
    "\n",
    "    # One-hot encode the SubwayStation column\n",
    "    df_test = pd.get_dummies(df_test, columns=['SubwayStation'])\n",
    "    df_test = df_test.drop('SubwayStation_no_subway_nearby', axis=1)\n",
    "\n",
    "    # Map the TimeToSubway column to numerical values\n",
    "    df_test['TimeToSubway'] = df_test['TimeToSubway'].map(time_to_subway_map)\n",
    "\n",
    "    # Map the HallwayType column to numerical values\n",
    "    if hallway_type_onehot:\n",
    "        df_test = pd.get_dummies(df_test, columns=['HallwayType'])\n",
    "    else:\n",
    "        df_test['HallwayType'] = df_test['HallwayType'].map(hallway_type_map)\n",
    "\n",
    "    # Mapping for HeatingType column\n",
    "    df_test['HeatingType'] = df_test['HeatingType'].map(heating_type_mapping)\n",
    "\n",
    "    # Mapping for AptManageType column\n",
    "    df_test['AptManageType'] = df_test['AptManageType'].map(apt_manage_type_mapping)\n",
    "\n",
    "    # Mapping for TimeToBusStop column\n",
    "    df_test['TimeToBusStop'] = df_test['TimeToBusStop'].map(time_to_bus_stop_mapping)\n",
    "\n",
    "    # Scale the numerical features using MinMaxScaler\n",
    "    df_test[num_cols] = scaler.transform(df_test[num_cols])\n",
    "\n",
    "    \n",
    "    # Convert the dataframes to numpy arrays\n",
    "    X_train = df_train.values\n",
    "    X_val = df_val.values\n",
    "    X_test = df_test.values\n",
    "\n",
    "    # Convert the target variables to numpy arrays\n",
    "    y_train = target_train.values\n",
    "    y_val = target_val.values\n",
    "\n",
    "    # Convert the numpy arrays to PyTorch tensors\n",
    "    X_train = torch.from_numpy(X_train).float()\n",
    "    X_val = torch.from_numpy(X_val).float()\n",
    "    X_test = torch.from_numpy(X_test).float()\n",
    "\n",
    "    y_train = torch.from_numpy(y_train).float()\n",
    "    y_val = torch.from_numpy(y_val).float()\n",
    "\n",
    "    # Create the train and validation datasets\n",
    "    train_ds = data.TensorDataset(X_train, y_train)\n",
    "    val_ds = data.TensorDataset(X_val, y_val)\n",
    "\n",
    "    # Create the train and validation data loaders\n",
    "    train_dl = data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_dl = data.DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "    return train_dl, val_dl, X_test, target_scaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'SalePrice_scaled'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/studia/SSNE/venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/studia/SSNE/venv/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/studia/SSNE/venv/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SalePrice_scaled'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m      3\u001b[0m \u001b[39m# Separate the target variable\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m y \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mSalePrice_scaled\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m      5\u001b[0m X \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mSalePrice\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSalePrice_scaled\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Split the data into training and validation sets\u001b[39;00m\n",
      "File \u001b[0;32m~/studia/SSNE/venv/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/studia/SSNE/venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SalePrice_scaled'"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Separate the target variable\n",
    "# y = df['SalePrice_scaled']\n",
    "# X = df.drop(['SalePrice', 'SalePrice_scaled'], axis=1)\n",
    "\n",
    "# # Split the data into training and validation sets\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = data.TensorDataset(torch.tensor(X_train.values, dtype=torch.float32), torch.tensor(y_train.values, dtype=torch.float32))\n",
    "# val_dataset = data.TensorDataset(torch.tensor(X_val.values, dtype=torch.float32), torch.tensor(y_val.values, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(valloader, model, target_scaler):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            inputs, labels_unscaled = data\n",
    "            labels_unscaled = labels_unscaled.reshape(-1, 1)\n",
    "            outputs = model(inputs)\n",
    "            outputs_unscaled = target_scaler.inverse_transform(outputs)\n",
    "            cheap_labels = labels_unscaled <= 100000\n",
    "            cheap_outputs = outputs_unscaled <= 100000\n",
    "            average_labels = (labels_unscaled > 100000) & (labels_unscaled <= 350000)\n",
    "            average_outputs = (outputs_unscaled > 100000) & (outputs_unscaled <= 350000)\n",
    "            expensive_labels = labels_unscaled > 350000\n",
    "            expensive_outputs = outputs_unscaled > 350000\n",
    "            cheap_correct = (cheap_labels * cheap_outputs).sum().item()\n",
    "            average_correct = (average_labels * average_outputs).sum().item()\n",
    "            expensive_correct = (expensive_labels * expensive_outputs).sum().item()\n",
    "            correct += cheap_correct + average_correct + expensive_correct\n",
    "            total += labels_unscaled.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train_epoch(dataloader, model, loss_fn, optimizer):\n",
    "    avg_loss = 0\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        pred = pred.reshape(-1)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss = loss.item()\n",
    "        avg_loss += loss\n",
    "    \n",
    "    avg_loss /= len(dataloader)\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, ReLU, LeakyReLU, Dropout\n",
    "\n",
    "def get_model(num_features, num_hidden, extra_layer=False, leaky_relu=False, dropout=False):\n",
    "    layers = []\n",
    "    layers.append(Linear(num_features, num_hidden))\n",
    "    if leaky_relu:\n",
    "        layers.append(LeakyReLU())\n",
    "    else:\n",
    "        layers.append(ReLU())\n",
    "    if extra_layer:\n",
    "        layers.append(Linear(num_hidden, num_hidden))\n",
    "        if leaky_relu:\n",
    "            layers.append(LeakyReLU())\n",
    "        else:\n",
    "            layers.append(ReLU())\n",
    "    if dropout:\n",
    "        layers.append(Dropout(0.2))\n",
    "    layers.append(Linear(num_hidden, 1))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def get_optimizer(model, lr=1e-4):\n",
    "    return torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def get_scheduler(optimizer, threshold=0.001, mode='min', factor=0.5, patience=5, verbose=True):\n",
    "    return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, threshold=threshold, mode=mode, factor=factor, patience=patience, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00053: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 00060: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch 00083: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch 00093: reducing learning rate of group 0 to 3.1250e-06.\n",
      "Done! 100. Training loss: 0.0059. Validation accuracy: 0.8448\n"
     ]
    }
   ],
   "source": [
    "# # train the model\n",
    "# epochs = 100\n",
    "# for t in range(epochs):\n",
    "#     avg_epoch_loss = train_epoch(train_data_loader, model, loss_fn, optimizer)\n",
    "#     acc = calc_accuracy(val_data_loader, model)\n",
    "#     scheduler.step(avg_epoch_loss)\n",
    "#     print(f\"Epoch {t+1}. Training loss: {avg_epoch_loss:.4f}. Validation accuracy: {acc:.4f}\", end=\"\\r\")\n",
    "# print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_train_dataset = data.TensorDataset(torch.tensor(X.values, dtype=torch.float32), torch.tensor(y.values, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hallway_type_onehot_options = [True, False]\n",
    "batch_size_options = [32, 64, 128]\n",
    "subway_cost_options = [0.2, 0.4, 0.6]\n",
    "extra_layer_options = [True, False]\n",
    "num_hidden_options = [32, 64, 128, 256]\n",
    "leaky_relu_options = [True, False]\n",
    "dropout_options = [True, False]\n",
    "lr_options = [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]\n",
    "threshold_options = [0.01, 0.001, 0.0001, 0.00001]\n",
    "factor_options = [0.5, 0.9]\n",
    "patience_options = [0, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_random_params():\n",
    "    return {\n",
    "        'hallway_type_onehot': random.choice(hallway_type_onehot_options),\n",
    "        'batch_size': random.choice(batch_size_options),\n",
    "        'subway_cost': random.choice(subway_cost_options),\n",
    "        'extra_layer': random.choice(extra_layer_options),\n",
    "        'num_hidden': random.choice(num_hidden_options),\n",
    "        'leaky_relu': random.choice(leaky_relu_options),\n",
    "        'dropout': random.choice(dropout_options),\n",
    "        'lr': random.choice(lr_options),\n",
    "        'threshold': random.choice(threshold_options),\n",
    "        'factor': random.choice(factor_options),\n",
    "        'patience': random.choice(patience_options)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031666021077678755"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'hallway_type_onehot': True, 'batch_size': 128, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 64, 'leaky_relu': False, 'dropout': True, 'lr': 0.001, 'threshold': 0.001, 'factor': 0.5, 'patience': 10}\n",
    "train_data_loader, val_data_loader, X_test, target_scaler = get_data_loaders(params['hallway_type_onehot'], params['batch_size'], params['subway_cost'])\n",
    "entry_size = X_test.shape[1]\n",
    "model = get_model(entry_size, params['num_hidden'], params['extra_layer'], params['leaky_relu'], params['dropout'])\n",
    "optimizer = get_optimizer(model, params['lr'])\n",
    "scheduler = get_scheduler(optimizer, params['threshold'], factor=params['factor'], patience=params['patience'])\n",
    "train_epoch(train_data_loader, model, loss_fn, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genetic_algorithm(population_size, num_generations, mutation_rate, crossover_rate, num_elites):\n",
    "    population = [get_random_params() for _ in range(population_size)]\n",
    "    for generation in range(num_generations):\n",
    "        print(f\"Generation {generation+1}\")\n",
    "        if generation > 0:\n",
    "            population = sorted(population, key=lambda x: x['accuracy'], reverse=True)\n",
    "        elites = population[:num_elites]\n",
    "        population = population[num_elites:]\n",
    "        for i in range(len(population)):\n",
    "            if random.random() < crossover_rate:\n",
    "                parent1 = random.choice(population)\n",
    "                parent2 = random.choice(population)\n",
    "                child = {}\n",
    "                for key in parent1:\n",
    "                    child[key] = random.choice([parent1[key], parent2[key]])\n",
    "                population[i] = child\n",
    "        for i in range(len(population)):\n",
    "            if random.random() < mutation_rate:\n",
    "                population[i] = get_random_params()\n",
    "        population = elites + population\n",
    "        for i in range(len(population)):\n",
    "            print(f\"Individual {i+1}\")\n",
    "            params = population[i]\n",
    "            train_data_loader, val_data_loader, X_test, target_scaler = get_data_loaders(params['hallway_type_onehot'], params['batch_size'], params['subway_cost'])\n",
    "            entry_size = X_test.shape[1]\n",
    "            model = get_model(entry_size, params['num_hidden'], params['extra_layer'], params['leaky_relu'], params['dropout'])\n",
    "            optimizer = get_optimizer(model, params['lr'])\n",
    "            scheduler = get_scheduler(optimizer, params['threshold'], factor=params['factor'], patience=params['patience'])\n",
    "            for t in range(100):\n",
    "                avg_epoch_loss = train_epoch(train_data_loader, model, loss_fn, optimizer)\n",
    "                acc = calc_accuracy(val_data_loader, model, target_scaler)\n",
    "                scheduler.step(avg_epoch_loss)\n",
    "                print(f\"Epoch {t+1}. Training loss: {avg_epoch_loss:.4f}. Validation accuracy: {acc:.4f}\", end=\"\\r\")\n",
    "            print(\"Done!\")\n",
    "            population[i]['accuracy'] = calc_accuracy(val_data_loader, model, target_scaler)\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 32, 'leaky_relu': True, 'dropout': True, 'lr': 0.0005, 'threshold': 0.001, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0061. Validation accuracy: 0.7442\n",
      "Done!\n",
      "Running 2th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 64, 'leaky_relu': True, 'dropout': True, 'lr': 0.001, 'threshold': 0.0001, 'factor': 0.5, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0074. Validation accuracy: 0.7406\n",
      "Done!\n",
      "Running 3th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.4, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': False, 'dropout': False, 'lr': 0.001, 'threshold': 0.001, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0020. Validation accuracy: 0.7370\n",
      "Done!\n",
      "Running 4th experiment with params {'hallway_type_onehot': True, 'batch_size': 32, 'subway_cost': 0.4, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': True, 'dropout': False, 'lr': 0.0005, 'threshold': 0.01, 'factor': 0.9, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0019. Validation accuracy: 0.7418\n",
      "Done!\n",
      "Running 5th experiment with params {'hallway_type_onehot': True, 'batch_size': 32, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 64, 'leaky_relu': False, 'dropout': True, 'lr': 0.0005, 'threshold': 0.001, 'factor': 0.9, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0049. Validation accuracy: 0.7358\n",
      "Done!\n",
      "Running 6th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 64, 'leaky_relu': True, 'dropout': False, 'lr': 1e-05, 'threshold': 0.01, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0053. Validation accuracy: 0.7463\n",
      "Done!\n",
      "Running 7th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.6, 'extra_layer': True, 'num_hidden': 128, 'leaky_relu': True, 'dropout': True, 'lr': 5e-05, 'threshold': 0.0001, 'factor': 0.9, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0024. Validation accuracy: 0.7341\n",
      "Done!\n",
      "Running 8th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 256, 'leaky_relu': False, 'dropout': False, 'lr': 1e-05, 'threshold': 0.01, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0135. Validation accuracy: 0.6824\n",
      "Done!\n",
      "Running 9th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.4, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': True, 'dropout': True, 'lr': 1e-05, 'threshold': 1e-05, 'factor': 0.5, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0289. Validation accuracy: 0.6873\n",
      "Done!\n",
      "Running 10th experiment with params {'hallway_type_onehot': True, 'batch_size': 128, 'subway_cost': 0.6, 'extra_layer': True, 'num_hidden': 32, 'leaky_relu': False, 'dropout': False, 'lr': 0.0001, 'threshold': 0.001, 'factor': 0.9, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0034. Validation accuracy: 0.7572\n",
      "Done!\n",
      "Running 11th experiment with params {'hallway_type_onehot': False, 'batch_size': 64, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 128, 'leaky_relu': True, 'dropout': False, 'lr': 0.001, 'threshold': 0.001, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0043. Validation accuracy: 0.7612\n",
      "Done!\n",
      "Running 12th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 32, 'leaky_relu': True, 'dropout': False, 'lr': 0.001, 'threshold': 0.0001, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0019. Validation accuracy: 0.7636\n",
      "Done!\n",
      "Running 13th experiment with params {'hallway_type_onehot': True, 'batch_size': 128, 'subway_cost': 0.6, 'extra_layer': True, 'num_hidden': 256, 'leaky_relu': True, 'dropout': False, 'lr': 0.0001, 'threshold': 1e-05, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0010. Validation accuracy: 0.7293\n",
      "Done!\n",
      "Running 14th experiment with params {'hallway_type_onehot': False, 'batch_size': 64, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': False, 'dropout': False, 'lr': 0.0005, 'threshold': 0.0001, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0010. Validation accuracy: 0.7467\n",
      "Done!\n",
      "Running 15th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.6, 'extra_layer': True, 'num_hidden': 64, 'leaky_relu': False, 'dropout': True, 'lr': 0.001, 'threshold': 1e-05, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0013. Validation accuracy: 0.7071\n",
      "Done!\n",
      "Running 16th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 256, 'leaky_relu': True, 'dropout': False, 'lr': 0.001, 'threshold': 0.0001, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0018. Validation accuracy: 0.7400\n",
      "Done!\n",
      "Running 17th experiment with params {'hallway_type_onehot': True, 'batch_size': 128, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 128, 'leaky_relu': False, 'dropout': True, 'lr': 5e-05, 'threshold': 0.001, 'factor': 0.9, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0034. Validation accuracy: 0.7545\n",
      "Done!\n",
      "Running 18th experiment with params {'hallway_type_onehot': False, 'batch_size': 64, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 128, 'leaky_relu': True, 'dropout': True, 'lr': 0.0001, 'threshold': 0.0001, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0053. Validation accuracy: 0.7382\n",
      "Done!\n",
      "Running 19th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 256, 'leaky_relu': False, 'dropout': True, 'lr': 0.001, 'threshold': 1e-05, 'factor': 0.5, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0023. Validation accuracy: 0.7531\n",
      "Done!\n",
      "Running 20th experiment with params {'hallway_type_onehot': False, 'batch_size': 64, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 64, 'leaky_relu': True, 'dropout': True, 'lr': 5e-05, 'threshold': 0.001, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0065. Validation accuracy: 0.7467\n",
      "Done!\n",
      "Running 21th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 32, 'leaky_relu': False, 'dropout': True, 'lr': 0.0001, 'threshold': 0.0001, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0143. Validation accuracy: 0.7188\n",
      "Done!\n",
      "Running 22th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.4, 'extra_layer': False, 'num_hidden': 64, 'leaky_relu': False, 'dropout': False, 'lr': 0.0005, 'threshold': 1e-05, 'factor': 0.5, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0022. Validation accuracy: 0.7582\n",
      "Done!\n",
      "Running 23th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 128, 'leaky_relu': False, 'dropout': False, 'lr': 0.001, 'threshold': 0.01, 'factor': 0.9, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0019. Validation accuracy: 0.7382\n",
      "Done!\n",
      "Running 24th experiment with params {'hallway_type_onehot': True, 'batch_size': 128, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': True, 'dropout': False, 'lr': 0.0005, 'threshold': 1e-05, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0012. Validation accuracy: 0.7733\n",
      "Done!\n",
      "Running 25th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 64, 'leaky_relu': True, 'dropout': False, 'lr': 5e-05, 'threshold': 0.0001, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0023. Validation accuracy: 0.7455\n",
      "Done!\n",
      "Running 26th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 256, 'leaky_relu': False, 'dropout': False, 'lr': 5e-05, 'threshold': 0.0001, 'factor': 0.9, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0047. Validation accuracy: 0.7467\n",
      "Done!\n",
      "Running 27th experiment with params {'hallway_type_onehot': False, 'batch_size': 64, 'subway_cost': 0.6, 'extra_layer': True, 'num_hidden': 64, 'leaky_relu': False, 'dropout': True, 'lr': 1e-05, 'threshold': 0.01, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0089. Validation accuracy: 0.7277\n",
      "Done!\n",
      "Running 28th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 32, 'leaky_relu': True, 'dropout': True, 'lr': 0.0001, 'threshold': 0.001, 'factor': 0.9, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0068. Validation accuracy: 0.7358\n",
      "Done!\n",
      "Running 29th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': False, 'dropout': True, 'lr': 0.001, 'threshold': 0.0001, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0015. Validation accuracy: 0.7224\n",
      "Done!\n",
      "Running 30th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': True, 'dropout': False, 'lr': 0.0005, 'threshold': 0.001, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0011. Validation accuracy: 0.7329\n",
      "Done!\n",
      "Running 31th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 32, 'leaky_relu': True, 'dropout': True, 'lr': 0.001, 'threshold': 1e-05, 'factor': 0.9, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0026. Validation accuracy: 0.7430\n",
      "Done!\n",
      "Running 32th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.4, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': True, 'dropout': False, 'lr': 0.001, 'threshold': 0.001, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0019. Validation accuracy: 0.7358\n",
      "Done!\n",
      "Running 33th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.4, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': True, 'dropout': True, 'lr': 5e-05, 'threshold': 0.001, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0045. Validation accuracy: 0.7370\n",
      "Done!\n",
      "Running 34th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.6, 'extra_layer': True, 'num_hidden': 64, 'leaky_relu': False, 'dropout': False, 'lr': 5e-05, 'threshold': 1e-05, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0026. Validation accuracy: 0.7212\n",
      "Done!\n",
      "Running 35th experiment with params {'hallway_type_onehot': True, 'batch_size': 32, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 32, 'leaky_relu': True, 'dropout': True, 'lr': 1e-05, 'threshold': 0.0001, 'factor': 0.9, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0073. Validation accuracy: 0.6648\n",
      "Done!\n",
      "Running 36th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.4, 'extra_layer': False, 'num_hidden': 32, 'leaky_relu': True, 'dropout': True, 'lr': 0.0005, 'threshold': 0.0001, 'factor': 0.9, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0032. Validation accuracy: 0.7770\n",
      "Done!\n",
      "Running 37th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 128, 'leaky_relu': False, 'dropout': True, 'lr': 5e-05, 'threshold': 0.01, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0043. Validation accuracy: 0.7364\n",
      "Done!\n",
      "Running 38th experiment with params {'hallway_type_onehot': True, 'batch_size': 128, 'subway_cost': 0.6, 'extra_layer': True, 'num_hidden': 32, 'leaky_relu': True, 'dropout': False, 'lr': 0.0001, 'threshold': 0.01, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0033. Validation accuracy: 0.7285\n",
      "Done!\n",
      "Running 39th experiment with params {'hallway_type_onehot': False, 'batch_size': 64, 'subway_cost': 0.4, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': True, 'dropout': False, 'lr': 1e-05, 'threshold': 0.01, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0087. Validation accuracy: 0.6842\n",
      "Done!\n",
      "Running 40th experiment with params {'hallway_type_onehot': True, 'batch_size': 32, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 64, 'leaky_relu': True, 'dropout': True, 'lr': 0.0001, 'threshold': 0.001, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0061. Validation accuracy: 0.7370\n",
      "Done!\n",
      "Running 41th experiment with params {'hallway_type_onehot': True, 'batch_size': 128, 'subway_cost': 0.4, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': True, 'dropout': True, 'lr': 0.0005, 'threshold': 0.001, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0025. Validation accuracy: 0.7527\n",
      "Done!\n",
      "Running 42th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.6, 'extra_layer': True, 'num_hidden': 128, 'leaky_relu': False, 'dropout': False, 'lr': 5e-05, 'threshold': 1e-05, 'factor': 0.9, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0011. Validation accuracy: 0.7236\n",
      "Done!\n",
      "Running 43th experiment with params {'hallway_type_onehot': True, 'batch_size': 128, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 256, 'leaky_relu': True, 'dropout': False, 'lr': 1e-05, 'threshold': 1e-05, 'factor': 0.5, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0059. Validation accuracy: 0.7358\n",
      "Done!\n",
      "Running 44th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 256, 'leaky_relu': True, 'dropout': True, 'lr': 1e-05, 'threshold': 0.001, 'factor': 0.5, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0179. Validation accuracy: 0.7487\n",
      "Done!\n",
      "Running 45th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': False, 'dropout': False, 'lr': 0.0001, 'threshold': 1e-05, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0013. Validation accuracy: 0.7378\n",
      "Done!\n",
      "Running 46th experiment with params {'hallway_type_onehot': True, 'batch_size': 32, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 128, 'leaky_relu': False, 'dropout': True, 'lr': 5e-05, 'threshold': 0.01, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0050. Validation accuracy: 0.7455\n",
      "Done!\n",
      "Running 47th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 64, 'leaky_relu': True, 'dropout': False, 'lr': 0.0005, 'threshold': 0.0001, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0019. Validation accuracy: 0.7545\n",
      "Done!\n",
      "Running 48th experiment with params {'hallway_type_onehot': True, 'batch_size': 128, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 256, 'leaky_relu': False, 'dropout': True, 'lr': 0.0005, 'threshold': 0.01, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0021. Validation accuracy: 0.7382\n",
      "Done!\n",
      "Running 49th experiment with params {'hallway_type_onehot': True, 'batch_size': 128, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 64, 'leaky_relu': True, 'dropout': False, 'lr': 0.0005, 'threshold': 0.0001, 'factor': 0.9, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0011. Validation accuracy: 0.7378\n",
      "Done!\n",
      "Running 50th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.4, 'extra_layer': False, 'num_hidden': 32, 'leaky_relu': False, 'dropout': True, 'lr': 5e-05, 'threshold': 0.01, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0156. Validation accuracy: 0.6945\n",
      "Done!\n",
      "Running 51th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.4, 'extra_layer': False, 'num_hidden': 32, 'leaky_relu': False, 'dropout': False, 'lr': 5e-05, 'threshold': 0.01, 'factor': 0.9, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0030. Validation accuracy: 0.7394\n",
      "Done!\n",
      "Running 52th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 256, 'leaky_relu': True, 'dropout': False, 'lr': 1e-05, 'threshold': 0.001, 'factor': 0.9, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0058. Validation accuracy: 0.7370\n",
      "Done!\n",
      "Running 53th experiment with params {'hallway_type_onehot': True, 'batch_size': 128, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 32, 'leaky_relu': True, 'dropout': True, 'lr': 0.0001, 'threshold': 0.0001, 'factor': 0.9, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0055. Validation accuracy: 0.7206\n",
      "Done!\n",
      "Running 54th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.4, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': False, 'dropout': False, 'lr': 5e-05, 'threshold': 0.001, 'factor': 0.5, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0025. Validation accuracy: 0.7497\n",
      "Done!\n",
      "Running 55th experiment with params {'hallway_type_onehot': False, 'batch_size': 64, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 128, 'leaky_relu': False, 'dropout': True, 'lr': 5e-05, 'threshold': 0.0001, 'factor': 0.5, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0071. Validation accuracy: 0.7297\n",
      "Done!\n",
      "Running 56th experiment with params {'hallway_type_onehot': False, 'batch_size': 64, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 64, 'leaky_relu': True, 'dropout': False, 'lr': 0.0001, 'threshold': 0.0001, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0020. Validation accuracy: 0.7618\n",
      "Done!\n",
      "Running 57th experiment with params {'hallway_type_onehot': True, 'batch_size': 32, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 64, 'leaky_relu': False, 'dropout': False, 'lr': 0.0001, 'threshold': 0.01, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0052. Validation accuracy: 0.7382\n",
      "Done!\n",
      "Running 58th experiment with params {'hallway_type_onehot': True, 'batch_size': 128, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 64, 'leaky_relu': False, 'dropout': True, 'lr': 5e-05, 'threshold': 0.001, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0090. Validation accuracy: 0.6752\n",
      "Done!\n",
      "Running 59th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.4, 'extra_layer': False, 'num_hidden': 256, 'leaky_relu': True, 'dropout': True, 'lr': 0.0005, 'threshold': 0.001, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0023. Validation accuracy: 0.7673\n",
      "Done!\n",
      "Running 60th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 256, 'leaky_relu': True, 'dropout': True, 'lr': 5e-05, 'threshold': 0.001, 'factor': 0.9, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0047. Validation accuracy: 0.7442\n",
      "Done!\n",
      "Running 61th experiment with params {'hallway_type_onehot': True, 'batch_size': 128, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 256, 'leaky_relu': True, 'dropout': False, 'lr': 5e-05, 'threshold': 0.001, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0020. Validation accuracy: 0.7642\n",
      "Done!\n",
      "Running 62th experiment with params {'hallway_type_onehot': True, 'batch_size': 128, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 256, 'leaky_relu': False, 'dropout': False, 'lr': 1e-05, 'threshold': 1e-05, 'factor': 0.9, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0058. Validation accuracy: 0.7333\n",
      "Done!\n",
      "Running 63th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.4, 'extra_layer': False, 'num_hidden': 256, 'leaky_relu': True, 'dropout': True, 'lr': 5e-05, 'threshold': 0.001, 'factor': 0.5, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0105. Validation accuracy: 0.6794\n",
      "Done!\n",
      "Running 64th experiment with params {'hallway_type_onehot': False, 'batch_size': 64, 'subway_cost': 0.6, 'extra_layer': True, 'num_hidden': 64, 'leaky_relu': True, 'dropout': True, 'lr': 0.0005, 'threshold': 0.0001, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0022. Validation accuracy: 0.7487\n",
      "Done!\n",
      "Running 65th experiment with params {'hallway_type_onehot': True, 'batch_size': 128, 'subway_cost': 0.4, 'extra_layer': False, 'num_hidden': 64, 'leaky_relu': False, 'dropout': False, 'lr': 0.001, 'threshold': 0.01, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0020. Validation accuracy: 0.7552\n",
      "Done!\n",
      "Running 66th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': False, 'dropout': True, 'lr': 5e-05, 'threshold': 0.001, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0054. Validation accuracy: 0.7677\n",
      "Done!\n",
      "Running 67th experiment with params {'hallway_type_onehot': False, 'batch_size': 64, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 64, 'leaky_relu': False, 'dropout': True, 'lr': 1e-05, 'threshold': 1e-05, 'factor': 0.9, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0146. Validation accuracy: 0.6630\n",
      "Done!\n",
      "Running 68th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.4, 'extra_layer': False, 'num_hidden': 32, 'leaky_relu': False, 'dropout': False, 'lr': 0.001, 'threshold': 0.0001, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0020. Validation accuracy: 0.7467\n",
      "Done!\n",
      "Running 69th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 256, 'leaky_relu': False, 'dropout': True, 'lr': 0.0001, 'threshold': 0.01, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0025. Validation accuracy: 0.7624\n",
      "Done!\n",
      "Running 70th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': True, 'dropout': False, 'lr': 5e-05, 'threshold': 0.0001, 'factor': 0.9, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0062. Validation accuracy: 0.7297\n",
      "Done!\n",
      "Running 71th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.4, 'extra_layer': False, 'num_hidden': 256, 'leaky_relu': True, 'dropout': True, 'lr': 0.0005, 'threshold': 1e-05, 'factor': 0.5, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0039. Validation accuracy: 0.7442\n",
      "Done!\n",
      "Running 72th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 32, 'leaky_relu': False, 'dropout': True, 'lr': 1e-05, 'threshold': 0.01, 'factor': 0.5, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0530. Validation accuracy: 0.6036\n",
      "Done!\n",
      "Running 73th experiment with params {'hallway_type_onehot': True, 'batch_size': 32, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 32, 'leaky_relu': False, 'dropout': True, 'lr': 1e-05, 'threshold': 0.01, 'factor': 0.5, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0283. Validation accuracy: 0.6630\n",
      "Done!\n",
      "Running 74th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 256, 'leaky_relu': True, 'dropout': False, 'lr': 0.0001, 'threshold': 0.01, 'factor': 0.5, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0050. Validation accuracy: 0.7515\n",
      "Done!\n",
      "Running 75th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.6, 'extra_layer': True, 'num_hidden': 128, 'leaky_relu': True, 'dropout': True, 'lr': 5e-05, 'threshold': 0.01, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0039. Validation accuracy: 0.7568\n",
      "Done!\n",
      "Running 76th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 64, 'leaky_relu': True, 'dropout': False, 'lr': 1e-05, 'threshold': 0.0001, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0102. Validation accuracy: 0.6782\n",
      "Done!\n",
      "Running 77th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.6, 'extra_layer': True, 'num_hidden': 128, 'leaky_relu': True, 'dropout': True, 'lr': 0.0005, 'threshold': 0.01, 'factor': 0.5, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0019. Validation accuracy: 0.7406\n",
      "Done!\n",
      "Running 78th experiment with params {'hallway_type_onehot': True, 'batch_size': 32, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': True, 'dropout': False, 'lr': 0.0001, 'threshold': 1e-05, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0048. Validation accuracy: 0.7624\n",
      "Done!\n",
      "Running 79th experiment with params {'hallway_type_onehot': False, 'batch_size': 64, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': True, 'dropout': False, 'lr': 0.0001, 'threshold': 0.001, 'factor': 0.9, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0049. Validation accuracy: 0.7491\n",
      "Done!\n",
      "Running 80th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': True, 'dropout': False, 'lr': 1e-05, 'threshold': 1e-05, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0094. Validation accuracy: 0.6727\n",
      "Done!\n",
      "Running 81th experiment with params {'hallway_type_onehot': True, 'batch_size': 32, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': True, 'dropout': True, 'lr': 0.0001, 'threshold': 1e-05, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0068. Validation accuracy: 0.7430\n",
      "Done!\n",
      "Running 82th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 32, 'leaky_relu': False, 'dropout': True, 'lr': 5e-05, 'threshold': 0.0001, 'factor': 0.5, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0140. Validation accuracy: 0.6679\n",
      "Done!\n",
      "Running 83th experiment with params {'hallway_type_onehot': False, 'batch_size': 64, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 64, 'leaky_relu': True, 'dropout': False, 'lr': 0.0005, 'threshold': 0.0001, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0044. Validation accuracy: 0.8024\n",
      "Done!\n",
      "Running 84th experiment with params {'hallway_type_onehot': True, 'batch_size': 128, 'subway_cost': 0.6, 'extra_layer': True, 'num_hidden': 256, 'leaky_relu': False, 'dropout': True, 'lr': 0.0005, 'threshold': 0.01, 'factor': 0.9, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0012. Validation accuracy: 0.7402\n",
      "Done!\n",
      "Running 85th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 128, 'leaky_relu': True, 'dropout': False, 'lr': 0.001, 'threshold': 0.0001, 'factor': 0.9, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0044. Validation accuracy: 0.7333\n",
      "Done!\n",
      "Running 86th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 128, 'leaky_relu': False, 'dropout': False, 'lr': 1e-05, 'threshold': 1e-05, 'factor': 0.9, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0062. Validation accuracy: 0.7261\n",
      "Done!\n",
      "Running 87th experiment with params {'hallway_type_onehot': True, 'batch_size': 128, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 64, 'leaky_relu': False, 'dropout': False, 'lr': 0.0005, 'threshold': 0.001, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0046. Validation accuracy: 0.7345\n",
      "Done!\n",
      "Running 88th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 32, 'leaky_relu': False, 'dropout': False, 'lr': 0.0001, 'threshold': 0.001, 'factor': 0.9, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0068. Validation accuracy: 0.7200\n",
      "Done!\n",
      "Running 89th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 64, 'leaky_relu': True, 'dropout': True, 'lr': 0.0001, 'threshold': 0.001, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0032. Validation accuracy: 0.7467\n",
      "Done!\n",
      "Running 90th experiment with params {'hallway_type_onehot': False, 'batch_size': 64, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 32, 'leaky_relu': True, 'dropout': True, 'lr': 5e-05, 'threshold': 0.001, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0139. Validation accuracy: 0.7317\n",
      "Done!\n",
      "Running 91th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 64, 'leaky_relu': True, 'dropout': True, 'lr': 0.0005, 'threshold': 0.001, 'factor': 0.9, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0058. Validation accuracy: 0.7515\n",
      "Done!\n",
      "Running 92th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': True, 'dropout': False, 'lr': 1e-05, 'threshold': 0.0001, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0060. Validation accuracy: 0.7451\n",
      "Done!\n",
      "Running 93th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 32, 'leaky_relu': False, 'dropout': False, 'lr': 0.0001, 'threshold': 0.01, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0052. Validation accuracy: 0.7515\n",
      "Done!\n",
      "Running 94th experiment with params {'hallway_type_onehot': True, 'batch_size': 32, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 64, 'leaky_relu': True, 'dropout': True, 'lr': 0.001, 'threshold': 1e-05, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0021. Validation accuracy: 0.7533\n",
      "Done!\n",
      "Running 95th experiment with params {'hallway_type_onehot': False, 'batch_size': 64, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 256, 'leaky_relu': True, 'dropout': False, 'lr': 5e-05, 'threshold': 0.0001, 'factor': 0.9, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0020. Validation accuracy: 0.7648\n",
      "Done!\n",
      "Running 96th experiment with params {'hallway_type_onehot': True, 'batch_size': 32, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 128, 'leaky_relu': True, 'dropout': True, 'lr': 0.0005, 'threshold': 0.01, 'factor': 0.9, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0046. Validation accuracy: 0.7333\n",
      "Done!\n",
      "Running 97th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.6, 'extra_layer': True, 'num_hidden': 256, 'leaky_relu': True, 'dropout': False, 'lr': 0.0001, 'threshold': 1e-05, 'factor': 0.5, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0010. Validation accuracy: 0.7442\n",
      "Done!\n",
      "Running 98th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 32, 'leaky_relu': False, 'dropout': True, 'lr': 0.001, 'threshold': 0.01, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0030. Validation accuracy: 0.7776\n",
      "Done!\n",
      "Running 99th experiment with params {'hallway_type_onehot': False, 'batch_size': 64, 'subway_cost': 0.6, 'extra_layer': True, 'num_hidden': 32, 'leaky_relu': False, 'dropout': False, 'lr': 0.0001, 'threshold': 0.001, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0015. Validation accuracy: 0.7491\n",
      "Done!\n",
      "Running 100th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 256, 'leaky_relu': False, 'dropout': False, 'lr': 5e-05, 'threshold': 0.01, 'factor': 0.9, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0051. Validation accuracy: 0.7442\n",
      "Done!\n",
      "Running 101th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.6, 'extra_layer': True, 'num_hidden': 256, 'leaky_relu': False, 'dropout': False, 'lr': 1e-05, 'threshold': 0.0001, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0014. Validation accuracy: 0.7523\n",
      "Done!\n",
      "Running 102th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.6, 'extra_layer': True, 'num_hidden': 64, 'leaky_relu': False, 'dropout': True, 'lr': 1e-05, 'threshold': 0.001, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0182. Validation accuracy: 0.7677\n",
      "Done!\n",
      "Running 103th experiment with params {'hallway_type_onehot': True, 'batch_size': 32, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 32, 'leaky_relu': True, 'dropout': False, 'lr': 0.001, 'threshold': 0.001, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0043. Validation accuracy: 0.7588\n",
      "Done!\n",
      "Running 104th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 128, 'leaky_relu': True, 'dropout': False, 'lr': 0.0001, 'threshold': 0.001, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0019. Validation accuracy: 0.7594\n",
      "Done!\n",
      "Running 105th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 32, 'leaky_relu': True, 'dropout': False, 'lr': 0.0001, 'threshold': 0.001, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0062. Validation accuracy: 0.7224\n",
      "Done!\n",
      "Running 106th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 64, 'leaky_relu': True, 'dropout': False, 'lr': 0.0005, 'threshold': 0.0001, 'factor': 0.5, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0014. Validation accuracy: 0.7442\n",
      "Done!\n",
      "Running 107th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 32, 'leaky_relu': True, 'dropout': True, 'lr': 5e-05, 'threshold': 0.01, 'factor': 0.9, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0110. Validation accuracy: 0.7495\n",
      "Done!\n",
      "Running 108th experiment with params {'hallway_type_onehot': True, 'batch_size': 32, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 256, 'leaky_relu': True, 'dropout': False, 'lr': 5e-05, 'threshold': 0.01, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0049. Validation accuracy: 0.7539\n",
      "Done!\n",
      "Running 109th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 32, 'leaky_relu': False, 'dropout': False, 'lr': 1e-05, 'threshold': 0.01, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0259. Validation accuracy: 0.6436\n",
      "Done!\n",
      "Running 110th experiment with params {'hallway_type_onehot': True, 'batch_size': 128, 'subway_cost': 0.6, 'extra_layer': True, 'num_hidden': 128, 'leaky_relu': False, 'dropout': False, 'lr': 1e-05, 'threshold': 0.01, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0115. Validation accuracy: 0.7644\n",
      "Done!\n",
      "Running 111th experiment with params {'hallway_type_onehot': False, 'batch_size': 64, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': True, 'dropout': False, 'lr': 5e-05, 'threshold': 0.0001, 'factor': 0.9, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0019. Validation accuracy: 0.7459\n",
      "Done!\n",
      "Running 112th experiment with params {'hallway_type_onehot': True, 'batch_size': 32, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 128, 'leaky_relu': True, 'dropout': True, 'lr': 0.0005, 'threshold': 0.01, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0024. Validation accuracy: 0.7515\n",
      "Done!\n",
      "Running 113th experiment with params {'hallway_type_onehot': False, 'batch_size': 64, 'subway_cost': 0.6, 'extra_layer': True, 'num_hidden': 256, 'leaky_relu': True, 'dropout': False, 'lr': 0.0001, 'threshold': 0.0001, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0010. Validation accuracy: 0.7430\n",
      "Done!\n",
      "Running 114th experiment with params {'hallway_type_onehot': False, 'batch_size': 64, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 32, 'leaky_relu': False, 'dropout': True, 'lr': 0.001, 'threshold': 0.001, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0052. Validation accuracy: 0.7366\n",
      "Done!\n",
      "Running 115th experiment with params {'hallway_type_onehot': True, 'batch_size': 128, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 64, 'leaky_relu': True, 'dropout': True, 'lr': 0.001, 'threshold': 0.0001, 'factor': 0.9, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0048. Validation accuracy: 0.8036\n",
      "Done!\n",
      "Running 116th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': True, 'dropout': True, 'lr': 5e-05, 'threshold': 0.01, 'factor': 0.9, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0080. Validation accuracy: 0.7523\n",
      "Done!\n",
      "Running 117th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.2, 'extra_layer': False, 'num_hidden': 64, 'leaky_relu': True, 'dropout': False, 'lr': 1e-05, 'threshold': 0.0001, 'factor': 0.9, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0106. Validation accuracy: 0.6739\n",
      "Done!\n",
      "Running 118th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.4, 'extra_layer': True, 'num_hidden': 32, 'leaky_relu': True, 'dropout': True, 'lr': 5e-05, 'threshold': 1e-05, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0050. Validation accuracy: 0.7085\n",
      "Done!\n",
      "Running 119th experiment with params {'hallway_type_onehot': False, 'batch_size': 64, 'subway_cost': 0.4, 'extra_layer': False, 'num_hidden': 256, 'leaky_relu': True, 'dropout': False, 'lr': 0.0005, 'threshold': 0.001, 'factor': 0.9, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0020. Validation accuracy: 0.7661\n",
      "Done!\n",
      "Running 120th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 64, 'leaky_relu': False, 'dropout': False, 'lr': 5e-05, 'threshold': 0.0001, 'factor': 0.9, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0031. Validation accuracy: 0.7455\n",
      "Done!\n",
      "Running 121th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 256, 'leaky_relu': True, 'dropout': False, 'lr': 1e-05, 'threshold': 0.0001, 'factor': 0.5, 'patience': 0}\n",
      "Epoch 100. Training loss: 0.0038. Validation accuracy: 0.7616\n",
      "Done!\n",
      "Running 122th experiment with params {'hallway_type_onehot': False, 'batch_size': 128, 'subway_cost': 0.4, 'extra_layer': False, 'num_hidden': 128, 'leaky_relu': True, 'dropout': True, 'lr': 0.0001, 'threshold': 0.01, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0056. Validation accuracy: 0.7170\n",
      "Done!\n",
      "Running 123th experiment with params {'hallway_type_onehot': True, 'batch_size': 64, 'subway_cost': 0.4, 'extra_layer': False, 'num_hidden': 256, 'leaky_relu': True, 'dropout': False, 'lr': 5e-05, 'threshold': 0.0001, 'factor': 0.5, 'patience': 10}\n",
      "Epoch 100. Training loss: 0.0022. Validation accuracy: 0.7503\n",
      "Done!\n",
      "Running 124th experiment with params {'hallway_type_onehot': False, 'batch_size': 32, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 32, 'leaky_relu': False, 'dropout': True, 'lr': 0.0001, 'threshold': 0.001, 'factor': 0.5, 'patience': 5}\n",
      "Epoch 100. Training loss: 0.0063. Validation accuracy: 0.7378\n",
      "Done!\n",
      "Running 125th experiment with params {'hallway_type_onehot': True, 'batch_size': 32, 'subway_cost': 0.6, 'extra_layer': False, 'num_hidden': 32, 'leaky_relu': True, 'dropout': False, 'lr': 0.0001, 'threshold': 0.01, 'factor': 0.5, 'patience': 0}\n",
      "Epoch 74. Training loss: 0.0049. Validation accuracy: 0.7572\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     15\u001b[0m     avg_epoch_loss \u001b[39m=\u001b[39m train_epoch(train_data_loader, model, loss_fn, optimizer)\n\u001b[0;32m---> 16\u001b[0m     acc \u001b[39m=\u001b[39m calc_accuracy(val_data_loader, model, target_scaler)\n\u001b[1;32m     17\u001b[0m     scheduler\u001b[39m.\u001b[39mstep(avg_epoch_loss)\n\u001b[1;32m     18\u001b[0m     \u001b[39mif\u001b[39;00m acc \u001b[39m>\u001b[39m best_acc:\n",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m, in \u001b[0;36mcalc_accuracy\u001b[0;34m(valloader, model, target_scaler)\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      5\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> 6\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m valloader:\n\u001b[1;32m      7\u001b[0m         inputs, labels_unscaled \u001b[39m=\u001b[39m data\n\u001b[1;32m      8\u001b[0m         labels_unscaled \u001b[39m=\u001b[39m labels_unscaled\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/studia/SSNE/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/studia/SSNE/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/studia/SSNE/venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/studia/SSNE/venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/studia/SSNE/venv/lib/python3.11/site-packages/torch/utils/data/dataset.py:196\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39;49m(tensor[index] \u001b[39mfor\u001b[39;49;00m tensor \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtensors)\n",
      "File \u001b[0;32m~/studia/SSNE/venv/lib/python3.11/site-packages/torch/utils/data/dataset.py:196\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(tensor[index] \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_params = None\n",
    "best_acc = 0\n",
    "\n",
    "for i in range(200):\n",
    "    params = get_random_params()\n",
    "    print(f\"Running {i+1}th experiment with params {params}\")\n",
    "    train_data_loader, val_data_loader, X_test, target_scaler = get_data_loaders(params['hallway_type_onehot'], params['batch_size'], params['subway_cost'])\n",
    "    entry_size = X_test.shape[1]\n",
    "    model = get_model(entry_size, params['num_hidden'], extra_layer=params['extra_layer'], leaky_relu=params['leaky_relu'], dropout=params['dropout'])\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = get_optimizer(model, lr=params['lr'])\n",
    "    scheduler = get_scheduler(optimizer, threshold=params['threshold'], factor=params['factor'], patience=params['patience'], verbose=False)\n",
    "    epochs = 100\n",
    "    for t in range(epochs):\n",
    "        avg_epoch_loss = train_epoch(train_data_loader, model, loss_fn, optimizer)\n",
    "        acc = calc_accuracy(val_data_loader, model, target_scaler)\n",
    "        scheduler.step(avg_epoch_loss)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_params = params\n",
    "        print(f\"Epoch {t+1}. Training loss: {avg_epoch_loss:.4f}. Validation accuracy: {acc:.4f}\", end=\"\\r\" if t < epochs - 1 else \"\\n\")\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hallway_type_onehot': True, 'batch_size': 128, 'subway_cost': 0.2, 'extra_layer': True, 'num_hidden': 64, 'leaky_relu': True, 'dropout': True, 'lr': 0.001, 'threshold': 0.0001, 'factor': 0.9, 'patience': 5}\n",
      "0.8303030303030303\n"
     ]
    }
   ],
   "source": [
    "print(best_params)\n",
    "print(best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1\n",
      "Individual 1\n",
      "Epoch 00076: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 00084: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Done! 100. Training loss: 0.0095. Validation accuracy: 0.7248\n",
      "Individual 2\n",
      "Epoch 00058: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Done! 100. Training loss: 0.0015. Validation accuracy: 0.7131\n",
      "Individual 3\n",
      "Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 00013: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 00014: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 00018: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 00019: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch 00022: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch 00023: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Epoch 00024: reducing learning rate of group 0 to 2.4414e-07.\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.2207e-07.\n",
      "Epoch 00026: reducing learning rate of group 0 to 6.1035e-08.\n",
      "Epoch 00027: reducing learning rate of group 0 to 3.0518e-08.\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.5259e-08.\n",
      "Done! 100. Training loss: 0.0057. Validation accuracy: 0.7455\n",
      "Individual 4\n",
      "Epoch 00029: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 00036: reducing learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 00039: reducing learning rate of group 0 to 7.2900e-04.\n",
      "Epoch 00041: reducing learning rate of group 0 to 6.5610e-04.\n",
      "Epoch 00043: reducing learning rate of group 0 to 5.9049e-04.\n",
      "Epoch 00046: reducing learning rate of group 0 to 5.3144e-04.\n",
      "Epoch 00047: reducing learning rate of group 0 to 4.7830e-04.\n",
      "Epoch 00050: reducing learning rate of group 0 to 4.3047e-04.\n",
      "Epoch 00052: reducing learning rate of group 0 to 3.8742e-04.\n",
      "Epoch 00054: reducing learning rate of group 0 to 3.4868e-04.\n",
      "Epoch 00055: reducing learning rate of group 0 to 3.1381e-04.\n",
      "Epoch 00056: reducing learning rate of group 0 to 2.8243e-04.\n",
      "Epoch 00059: reducing learning rate of group 0 to 2.5419e-04.\n",
      "Epoch 00060: reducing learning rate of group 0 to 2.2877e-04.\n",
      "Epoch 00061: reducing learning rate of group 0 to 2.0589e-04.\n",
      "Epoch 00062: reducing learning rate of group 0 to 1.8530e-04.\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.6677e-04.\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.5009e-04.\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.3509e-04.\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.2158e-04.\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0942e-04.\n",
      "Epoch 00069: reducing learning rate of group 0 to 9.8477e-05.\n",
      "Epoch 00070: reducing learning rate of group 0 to 8.8629e-05.\n",
      "Epoch 00072: reducing learning rate of group 0 to 7.9766e-05.\n",
      "Epoch 00073: reducing learning rate of group 0 to 7.1790e-05.\n",
      "Epoch 00074: reducing learning rate of group 0 to 6.4611e-05.\n",
      "Epoch 00075: reducing learning rate of group 0 to 5.8150e-05.\n",
      "Epoch 00076: reducing learning rate of group 0 to 5.2335e-05.\n",
      "Epoch 00077: reducing learning rate of group 0 to 4.7101e-05.\n",
      "Epoch 00078: reducing learning rate of group 0 to 4.2391e-05.\n",
      "Epoch 00079: reducing learning rate of group 0 to 3.8152e-05.\n",
      "Epoch 00080: reducing learning rate of group 0 to 3.4337e-05.\n",
      "Epoch 00081: reducing learning rate of group 0 to 3.0903e-05.\n",
      "Epoch 00082: reducing learning rate of group 0 to 2.7813e-05.\n",
      "Epoch 00083: reducing learning rate of group 0 to 2.5032e-05.\n",
      "Epoch 00085: reducing learning rate of group 0 to 2.2528e-05.\n",
      "Epoch 00086: reducing learning rate of group 0 to 2.0276e-05.\n",
      "Epoch 00087: reducing learning rate of group 0 to 1.8248e-05.\n",
      "Epoch 00088: reducing learning rate of group 0 to 1.6423e-05.\n",
      "Epoch 00089: reducing learning rate of group 0 to 1.4781e-05.\n",
      "Epoch 00090: reducing learning rate of group 0 to 1.3303e-05.\n",
      "Epoch 00091: reducing learning rate of group 0 to 1.1973e-05.\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.0775e-05.\n",
      "Epoch 00093: reducing learning rate of group 0 to 9.6977e-06.\n",
      "Epoch 00094: reducing learning rate of group 0 to 8.7280e-06.\n",
      "Epoch 00095: reducing learning rate of group 0 to 7.8552e-06.\n",
      "Epoch 00096: reducing learning rate of group 0 to 7.0697e-06.\n",
      "Epoch 00097: reducing learning rate of group 0 to 6.3627e-06.\n",
      "Epoch 00098: reducing learning rate of group 0 to 5.7264e-06.\n",
      "Epoch 00099: reducing learning rate of group 0 to 5.1538e-06.\n",
      "Epoch 00100: reducing learning rate of group 0 to 4.6384e-06.\n",
      "Done! 100. Training loss: 0.0013. Validation accuracy: 0.7196\n",
      "Individual 5\n",
      "Done! 100. Training loss: 0.0043. Validation accuracy: 0.7212\n",
      "Individual 6\n",
      "Done! 100. Training loss: 0.0037. Validation accuracy: 0.7479\n",
      "Individual 7\n",
      "Epoch 00066: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 00087: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Done! 100. Training loss: 0.0048. Validation accuracy: 0.7491\n",
      "Individual 8\n",
      "Epoch 00031: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 00053: reducing learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 00059: reducing learning rate of group 0 to 7.2900e-04.\n",
      "Epoch 00067: reducing learning rate of group 0 to 6.5610e-04.\n",
      "Epoch 00073: reducing learning rate of group 0 to 5.9049e-04.\n",
      "Epoch 00079: reducing learning rate of group 0 to 5.3144e-04.\n",
      "Epoch 00086: reducing learning rate of group 0 to 4.7830e-04.\n",
      "Epoch 00092: reducing learning rate of group 0 to 4.3047e-04.\n",
      "Done! 100. Training loss: 0.0009. Validation accuracy: 0.7434\n",
      "Individual 9\n",
      "Epoch 00079: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch 00095: reducing learning rate of group 0 to 2.5000e-06.\n",
      "Done! 100. Training loss: 0.0070. Validation accuracy: 0.7133\n",
      "Individual 10\n",
      "Epoch 00087: reducing learning rate of group 0 to 4.5000e-05.\n",
      "Epoch 00096: reducing learning rate of group 0 to 4.0500e-05.\n",
      "Done! 100. Training loss: 0.0067. Validation accuracy: 0.7370\n",
      "Generation 2\n",
      "Individual 1\n",
      "Epoch 00084: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 00093: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Done! 100. Training loss: 0.0048. Validation accuracy: 0.7685\n",
      "Individual 2\n",
      "Done! 100. Training loss: 0.0076. Validation accuracy: 0.7620\n",
      "Individual 3\n",
      "Epoch 00071: reducing learning rate of group 0 to 4.5000e-05.\n",
      "Epoch 00080: reducing learning rate of group 0 to 4.0500e-05.\n",
      "Epoch 00089: reducing learning rate of group 0 to 3.6450e-05.\n",
      "Epoch 00099: reducing learning rate of group 0 to 3.2805e-05.\n",
      "Done! 100. Training loss: 0.0053. Validation accuracy: 0.7503\n",
      "Individual 4\n",
      "Done! 100. Training loss: 0.0025. Validation accuracy: 0.7317\n",
      "Individual 5\n",
      "Done! 100. Training loss: 0.0028. Validation accuracy: 0.7412\n",
      "Individual 6\n",
      "Epoch 00060: reducing learning rate of group 0 to 4.5000e-05.\n",
      "Epoch 00074: reducing learning rate of group 0 to 4.0500e-05.\n",
      "Epoch 00086: reducing learning rate of group 0 to 3.6450e-05.\n",
      "Epoch 00092: reducing learning rate of group 0 to 3.2805e-05.\n",
      "Done! 100. Training loss: 0.0053. Validation accuracy: 0.7842\n",
      "Individual 7\n",
      "Epoch 00100: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Done! 100. Training loss: 0.0021. Validation accuracy: 0.7285\n",
      "Individual 8\n",
      "Epoch 00040: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch 00068: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch 00079: reducing learning rate of group 0 to 3.1250e-06.\n",
      "Epoch 00085: reducing learning rate of group 0 to 1.5625e-06.\n",
      "Epoch 00091: reducing learning rate of group 0 to 7.8125e-07.\n",
      "Epoch 00097: reducing learning rate of group 0 to 3.9063e-07.\n",
      "Done! 100. Training loss: 0.0048. Validation accuracy: 0.7564\n",
      "Individual 9\n",
      "Epoch 00072: reducing learning rate of group 0 to 4.5000e-04.\n",
      "Epoch 00084: reducing learning rate of group 0 to 4.0500e-04.\n",
      "Epoch 00092: reducing learning rate of group 0 to 3.6450e-04.\n",
      "Done! 100. Training loss: 0.0010. Validation accuracy: 0.7459\n",
      "Individual 10\n",
      "Done! 100. Training loss: 0.0103. Validation accuracy: 0.7471\n",
      "Generation 3\n",
      "Individual 1\n",
      "Epoch 00066: reducing learning rate of group 0 to 4.5000e-05.\n",
      "Epoch 00084: reducing learning rate of group 0 to 4.0500e-05.\n",
      "Done! 100. Training loss: 0.0051. Validation accuracy: 0.7382\n",
      "Individual 2\n",
      "Done! 100. Training loss: 0.0048. Validation accuracy: 0.7479\n",
      "Individual 3\n",
      "Epoch 00039: reducing learning rate of group 0 to 4.5000e-04.\n",
      "Epoch 00046: reducing learning rate of group 0 to 4.0500e-04.\n",
      "Epoch 00049: reducing learning rate of group 0 to 3.6450e-04.\n",
      "Epoch 00060: reducing learning rate of group 0 to 3.2805e-04.\n",
      "Epoch 00061: reducing learning rate of group 0 to 2.9525e-04.\n",
      "Epoch 00067: reducing learning rate of group 0 to 2.6572e-04.\n",
      "Epoch 00071: reducing learning rate of group 0 to 2.3915e-04.\n",
      "Epoch 00075: reducing learning rate of group 0 to 2.1523e-04.\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.9371e-04.\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.7434e-04.\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.5691e-04.\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.4121e-04.\n",
      "Epoch 00085: reducing learning rate of group 0 to 1.2709e-04.\n",
      "Epoch 00087: reducing learning rate of group 0 to 1.1438e-04.\n",
      "Epoch 00089: reducing learning rate of group 0 to 1.0295e-04.\n",
      "Epoch 00091: reducing learning rate of group 0 to 9.2651e-05.\n",
      "Epoch 00094: reducing learning rate of group 0 to 8.3386e-05.\n",
      "Epoch 00095: reducing learning rate of group 0 to 7.5047e-05.\n",
      "Epoch 00097: reducing learning rate of group 0 to 6.7543e-05.\n",
      "Epoch 00098: reducing learning rate of group 0 to 6.0788e-05.\n",
      "Epoch 00099: reducing learning rate of group 0 to 5.4709e-05.\n",
      "Done! 100. Training loss: 0.0011. Validation accuracy: 0.7414\n",
      "Individual 4\n",
      "Done! 100. Training loss: 0.0123. Validation accuracy: 0.7503\n",
      "Individual 5\n",
      "Done! 100. Training loss: 0.0315. Validation accuracy: 0.7648\n",
      "Individual 6\n",
      "Epoch 00077: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Done! 100. Training loss: 0.0066. Validation accuracy: 0.7681\n",
      "Individual 7\n",
      "Epoch 00043: reducing learning rate of group 0 to 4.5000e-04.\n",
      "Epoch 00054: reducing learning rate of group 0 to 4.0500e-04.\n",
      "Epoch 00062: reducing learning rate of group 0 to 3.6450e-04.\n",
      "Epoch 00080: reducing learning rate of group 0 to 3.2805e-04.\n",
      "Epoch 00091: reducing learning rate of group 0 to 2.9525e-04.\n",
      "Epoch 00097: reducing learning rate of group 0 to 2.6572e-04.\n",
      "Done! 100. Training loss: 0.0009. Validation accuracy: 0.7446\n",
      "Individual 8\n",
      "Done! 100. Training loss: 0.0030. Validation accuracy: 0.7630\n",
      "Individual 9\n",
      "Done! 100. Training loss: 0.0148. Validation accuracy: 0.7386\n",
      "Individual 10\n",
      "Epoch 00095: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Done! 100. Training loss: 0.0025. Validation accuracy: 0.7463\n",
      "Generation 4\n",
      "Individual 1\n",
      "Done! 100. Training loss: 0.0072. Validation accuracy: 0.7580\n",
      "Individual 2\n",
      "Done! 100. Training loss: 0.0184. Validation accuracy: 0.7673\n",
      "Individual 3\n",
      "Done! 100. Training loss: 0.0026. Validation accuracy: 0.7527\n",
      "Individual 4\n",
      "Epoch 00088: reducing learning rate of group 0 to 9.0000e-06.\n",
      "Epoch 00097: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Done! 100. Training loss: 0.0095. Validation accuracy: 0.7196\n",
      "Individual 5\n",
      "Done! 100. Training loss: 0.0048. Validation accuracy: 0.7503\n",
      "Individual 6\n",
      "Done! 100. Training loss: 0.0021. Validation accuracy: 0.7511\n",
      "Individual 7\n",
      "Epoch 00054: reducing learning rate of group 0 to 4.5000e-05.\n",
      "Epoch 00064: reducing learning rate of group 0 to 4.0500e-05.\n",
      "Epoch 00085: reducing learning rate of group 0 to 3.6450e-05.\n",
      "Epoch 00096: reducing learning rate of group 0 to 3.2805e-05.\n",
      "Done! 100. Training loss: 0.0052. Validation accuracy: 0.7515\n",
      "Individual 8\n",
      "Done! 100. Training loss: 0.0159. Validation accuracy: 0.7657\n",
      "Individual 9\n",
      "Done! 100. Training loss: 0.0104. Validation accuracy: 0.7362\n",
      "Individual 10\n",
      "Done! 100. Training loss: 0.0143. Validation accuracy: 0.7515\n",
      "Generation 5\n",
      "Individual 1\n",
      "Done! 100. Training loss: 0.0875. Validation accuracy: 0.0614\n",
      "Individual 2\n",
      "Done! 100. Training loss: 0.0118. Validation accuracy: 0.7188\n",
      "Individual 3\n",
      "Done! 100. Training loss: 0.0063. Validation accuracy: 0.7511\n",
      "Individual 4\n",
      "Epoch 00089: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Done! 100. Training loss: 0.0020. Validation accuracy: 0.7547\n",
      "Individual 5\n",
      "Epoch 00081: reducing learning rate of group 0 to 4.5000e-05.\n",
      "Epoch 00089: reducing learning rate of group 0 to 4.0500e-05.\n",
      "Done! 100. Training loss: 0.0053. Validation accuracy: 0.7576\n",
      "Individual 6\n",
      "Done! 100. Training loss: 0.0119. Validation accuracy: 0.7737\n",
      "Individual 7\n",
      "Epoch 00084: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Done! 100. Training loss: 0.0024. Validation accuracy: 0.7495\n",
      "Individual 8\n",
      "Done! 100. Training loss: 0.0084. Validation accuracy: 0.7325\n",
      "Individual 9\n",
      "Done! 100. Training loss: 0.0108. Validation accuracy: 0.7483\n",
      "Individual 10\n",
      "Done! 100. Training loss: 0.0100. Validation accuracy: 0.7341\n",
      "Generation 6\n",
      "Individual 1\n",
      "Done! 100. Training loss: 0.0087. Validation accuracy: 0.7426\n",
      "Individual 2\n",
      "Epoch 00046: reducing learning rate of group 0 to 4.5000e-05.\n",
      "Epoch 00057: reducing learning rate of group 0 to 4.0500e-05.\n",
      "Epoch 00082: reducing learning rate of group 0 to 3.6450e-05.\n",
      "Done! 100. Training loss: 0.0050. Validation accuracy: 0.7467\n",
      "Individual 3\n",
      "Epoch 00098: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Done! 100. Training loss: 0.0010. Validation accuracy: 0.7325\n",
      "Individual 4\n",
      "Epoch 00067: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch 00074: reducing learning rate of group 0 to 2.5000e-06.\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.2500e-06.\n",
      "Epoch 00091: reducing learning rate of group 0 to 6.2500e-07.\n",
      "Done! 100. Training loss: 0.0090. Validation accuracy: 0.7374\n",
      "Individual 5\n",
      "Epoch 00074: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch 00093: reducing learning rate of group 0 to 2.5000e-06.\n",
      "Done! 100. Training loss: 0.0029. Validation accuracy: 0.7531\n",
      "Individual 6\n",
      "Done! 100. Training loss: 0.0104. Validation accuracy: 0.7467\n",
      "Individual 7\n",
      "Epoch 00090: reducing learning rate of group 0 to 9.0000e-06.\n",
      "Done! 100. Training loss: 0.0152. Validation accuracy: 0.7624\n",
      "Individual 8\n",
      "Done! 100. Training loss: 0.0097. Validation accuracy: 0.7556\n",
      "Individual 9\n",
      "Done! 100. Training loss: 0.0024. Validation accuracy: 0.7596\n",
      "Individual 10\n",
      "Done! 100. Training loss: 0.0816. Validation accuracy: 0.0614\n",
      "Generation 7\n",
      "Individual 1\n",
      "Done! 100. Training loss: 0.0137. Validation accuracy: 0.7345\n",
      "Individual 2\n",
      "Done! 100. Training loss: 0.0020. Validation accuracy: 0.7543\n",
      "Individual 3\n",
      "Done! 100. Training loss: 0.0053. Validation accuracy: 0.7491\n",
      "Individual 4\n",
      "Epoch 00064: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 00073: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch 00087: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch 00093: reducing learning rate of group 0 to 3.1250e-06.\n",
      "Done! 100. Training loss: 0.0010. Validation accuracy: 0.7394\n",
      "Individual 5\n",
      "Epoch 00073: reducing learning rate of group 0 to 4.5000e-05.\n",
      "Epoch 00088: reducing learning rate of group 0 to 4.0500e-05.\n",
      "Epoch 00094: reducing learning rate of group 0 to 3.6450e-05.\n",
      "Done! 100. Training loss: 0.0054. Validation accuracy: 0.7527\n",
      "Individual 6\n",
      "Done! 100. Training loss: 0.0136. Validation accuracy: 0.7677\n",
      "Individual 7\n",
      "Done! 100. Training loss: 0.0117. Validation accuracy: 0.7442\n",
      "Individual 8\n",
      "Done! 100. Training loss: 0.0074. Validation accuracy: 0.7608\n",
      "Individual 9\n",
      "Done! 100. Training loss: 0.0650. Validation accuracy: 0.2772\n",
      "Individual 10\n",
      "Done! 100. Training loss: 0.0203. Validation accuracy: 0.7673\n",
      "Generation 8\n",
      "Individual 1\n",
      "Epoch 00076: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch 00085: reducing learning rate of group 0 to 2.5000e-06.\n",
      "Done! 100. Training loss: 0.0095. Validation accuracy: 0.7362\n",
      "Individual 2\n",
      "Done! 100. Training loss: 0.0305. Validation accuracy: 0.7697\n",
      "Individual 3\n",
      "Epoch 00092: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Done! 100. Training loss: 0.0125. Validation accuracy: 0.7374\n",
      "Individual 4\n",
      "Done! 100. Training loss: 0.0053. Validation accuracy: 0.7418\n",
      "Individual 5\n",
      "Epoch 00081: reducing learning rate of group 0 to 4.5000e-05.\n",
      "Done! 100. Training loss: 0.0052. Validation accuracy: 0.7515\n",
      "Individual 6\n",
      "Done! 100. Training loss: 0.0051. Validation accuracy: 0.7455\n",
      "Individual 7\n",
      "Done! 100. Training loss: 0.0122. Validation accuracy: 0.7305\n",
      "Individual 8\n",
      "Epoch 00072: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 00084: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 00091: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch 00097: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Done! 100. Training loss: 0.0010. Validation accuracy: 0.7236\n",
      "Individual 9\n",
      "Epoch 00060: reducing learning rate of group 0 to 9.0000e-06.\n",
      "Done! 100. Training loss: 0.0120. Validation accuracy: 0.7305\n",
      "Individual 10\n",
      "Epoch 00022: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 00025: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch 00031: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch 00032: reducing learning rate of group 0 to 3.1250e-06.\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.5625e-06.\n",
      "Epoch 00034: reducing learning rate of group 0 to 7.8125e-07.\n",
      "Epoch 00036: reducing learning rate of group 0 to 3.9063e-07.\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.9531e-07.\n",
      "Epoch 00039: reducing learning rate of group 0 to 9.7656e-08.\n",
      "Epoch 00040: reducing learning rate of group 0 to 4.8828e-08.\n",
      "Epoch 00041: reducing learning rate of group 0 to 2.4414e-08.\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.2207e-08.\n",
      "Done! 100. Training loss: 0.0050. Validation accuracy: 0.7382\n",
      "Generation 9\n",
      "Individual 1\n",
      "Done! 100. Training loss: 0.0341. Validation accuracy: 0.6432\n",
      "Individual 2\n",
      "Epoch 00041: reducing learning rate of group 0 to 4.5000e-05.\n",
      "Epoch 00064: reducing learning rate of group 0 to 4.0500e-05.\n",
      "Epoch 00079: reducing learning rate of group 0 to 3.6450e-05.\n",
      "Epoch 00085: reducing learning rate of group 0 to 3.2805e-05.\n",
      "Epoch 00097: reducing learning rate of group 0 to 2.9525e-05.\n",
      "Done! 100. Training loss: 0.0054. Validation accuracy: 0.7503\n",
      "Individual 3\n",
      "Epoch 00093: reducing learning rate of group 0 to 4.5000e-05.\n",
      "Done! 100. Training loss: 0.0055. Validation accuracy: 0.7560\n",
      "Individual 4\n",
      "Done! 100. Training loss: 0.0078. Validation accuracy: 0.7188\n",
      "Individual 5\n",
      "Epoch 00081: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch 00093: reducing learning rate of group 0 to 2.5000e-06.\n",
      "Done! 100. Training loss: 0.0144. Validation accuracy: 0.7200\n",
      "Individual 6\n",
      "Done! 100. Training loss: 0.0201. Validation accuracy: 0.7612\n",
      "Individual 7\n",
      "Done! 100. Training loss: 0.0147. Validation accuracy: 0.7552\n",
      "Individual 8\n",
      "Done! 100. Training loss: 0.0104. Validation accuracy: 0.7325\n",
      "Individual 9\n",
      "Done! 100. Training loss: 0.0151. Validation accuracy: 0.7261\n",
      "Individual 10\n",
      "Done! 100. Training loss: 0.0061. Validation accuracy: 0.7273\n",
      "Generation 10\n",
      "Individual 1\n",
      "Epoch 00070: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch 00077: reducing learning rate of group 0 to 2.5000e-06.\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.2500e-06.\n",
      "Done! 100. Training loss: 0.0212. Validation accuracy: 0.7317\n",
      "Individual 2\n",
      "Epoch 00071: reducing learning rate of group 0 to 4.5000e-05.\n",
      "Done! 100. Training loss: 0.0077. Validation accuracy: 0.7576\n",
      "Individual 3\n",
      "Epoch 00085: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch 00091: reducing learning rate of group 0 to 2.5000e-06.\n",
      "Epoch 00097: reducing learning rate of group 0 to 1.2500e-06.\n",
      "Done! 100. Training loss: 0.0131. Validation accuracy: 0.7426\n",
      "Individual 4\n",
      "Epoch 00041: reducing learning rate of group 0 to 4.5000e-05.\n",
      "Epoch 00063: reducing learning rate of group 0 to 4.0500e-05.\n",
      "Epoch 00070: reducing learning rate of group 0 to 3.6450e-05.\n",
      "Epoch 00078: reducing learning rate of group 0 to 3.2805e-05.\n",
      "Epoch 00091: reducing learning rate of group 0 to 2.9525e-05.\n",
      "Done! 100. Training loss: 0.0052. Validation accuracy: 0.7539\n",
      "Individual 5\n",
      "Epoch 00054: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch 00079: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch 00085: reducing learning rate of group 0 to 3.1250e-06.\n",
      "Epoch 00091: reducing learning rate of group 0 to 1.5625e-06.\n",
      "Done! 100. Training loss: 0.0072. Validation accuracy: 0.7285\n",
      "Individual 6\n",
      "Done! 100. Training loss: 0.0363. Validation accuracy: 0.7301\n",
      "Individual 7\n",
      "Done! 100. Training loss: 0.0239. Validation accuracy: 0.7325\n",
      "Individual 8\n",
      "Epoch 00092: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Done! 100. Training loss: 0.0154. Validation accuracy: 0.7131\n",
      "Individual 9\n",
      "Epoch 00099: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Done! 100. Training loss: 0.0102. Validation accuracy: 0.7200\n",
      "Individual 10\n",
      "Epoch 00078: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 00085: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch 00096: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Done! 100. Training loss: 0.0090. Validation accuracy: 0.7273\n",
      "Generation 11\n",
      "Individual 1\n",
      "Epoch 00071: reducing learning rate of group 0 to 4.5000e-05.\n",
      "Epoch 00086: reducing learning rate of group 0 to 4.0500e-05.\n",
      "Done! 100. Training loss: 0.0076. Validation accuracy: 0.7244\n",
      "Individual 2\n",
      "Epoch 00058: reducing learning rate of group 0 to 4.5000e-05.\n",
      "Epoch 00071: reducing learning rate of group 0 to 4.0500e-05.\n",
      "Epoch 00079: reducing learning rate of group 0 to 3.6450e-05.\n",
      "Done! 100. Training loss: 0.0052. Validation accuracy: 0.7479\n",
      "Individual 3\n",
      "Done! 100. Training loss: 0.0197. Validation accuracy: 0.7475\n",
      "Individual 4\n",
      "Epoch 00087: reducing learning rate of group 0 to 9.0000e-06.\n",
      "Done! 100. Training loss: 0.0225. Validation accuracy: 0.6958\n",
      "Individual 5\n",
      "Epoch 00097: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Done! 100. Training loss: 0.0119. Validation accuracy: 0.7333\n",
      "Individual 6\n",
      "Epoch 00068: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 00089: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch 00098: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Done! 100. Training loss: 0.0085. Validation accuracy: 0.7188\n",
      "Individual 7\n",
      "Epoch 00091: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Done! 100. Training loss: 0.0079. Validation accuracy: 0.7321\n",
      "Individual 8\n",
      "Done! 100. Training loss: 0.0073. Validation accuracy: 0.7321\n",
      "Individual 9\n",
      "Done! 100. Training loss: 0.0081. Validation accuracy: 0.7345\n",
      "Individual 10\n",
      "Epoch 00077: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Done! 100. Training loss: 0.0145. Validation accuracy: 0.7349\n",
      "Generation 12\n",
      "Individual 1\n",
      "Epoch 00063: reducing learning rate of group 0 to 4.5000e-05.\n",
      "Epoch 00072: reducing learning rate of group 0 to 4.0500e-05.\n",
      "Epoch 00082: reducing learning rate of group 0 to 3.6450e-05.\n",
      "Epoch 00095: reducing learning rate of group 0 to 3.2805e-05.\n",
      "Done! 100. Training loss: 0.0050. Validation accuracy: 0.7576\n",
      "Individual 2\n",
      "Epoch 00078: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch 00094: reducing learning rate of group 0 to 2.5000e-06.\n",
      "Done! 100. Training loss: 0.0173. Validation accuracy: 0.7438\n",
      "Individual 3\n",
      "Epoch 00077: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Done! 100. Training loss: 0.0196. Validation accuracy: 0.7463\n",
      "Individual 4\n",
      "Epoch 00085: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch 00091: reducing learning rate of group 0 to 2.5000e-06.\n",
      "Done! 100. Training loss: 0.0099. Validation accuracy: 0.7438\n",
      "Individual 5\n",
      "Done! 100. Training loss: 0.0158. Validation accuracy: 0.7588\n",
      "Individual 6\n",
      "Epoch 00067: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 00095: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Done! 100. Training loss: 0.0063. Validation accuracy: 0.7394\n",
      "Individual 7\n",
      "Epoch 00077: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 00085: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch 00100: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Done! 100. Training loss: 0.0079. Validation accuracy: 0.7273\n",
      "Individual 8\n",
      "Done! 100. Training loss: 0.0066. Validation accuracy: 0.7402\n",
      "Individual 9\n",
      "Epoch 00094: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Done! 100. Training loss: 0.0164. Validation accuracy: 0.7410\n",
      "Individual 10\n",
      "Done! 100. Training loss: 0.0125. Validation accuracy: 0.7358\n",
      "Generation 13\n",
      "Individual 1\n",
      "Epoch 00078: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Done! 100. Training loss: 0.0144. Validation accuracy: 0.7192\n",
      "Individual 2\n",
      "Epoch 00039: reducing learning rate of group 0 to 4.5000e-05.\n",
      "Epoch 00053: reducing learning rate of group 0 to 4.0500e-05.\n",
      "Epoch 00072: reducing learning rate of group 0 to 3.6450e-05.\n",
      "Epoch 00086: reducing learning rate of group 0 to 3.2805e-05.\n",
      "Done! 100. Training loss: 0.0051. Validation accuracy: 0.7527\n",
      "Individual 3\n",
      "Done! 100. Training loss: 0.0201. Validation accuracy: 0.7640\n",
      "Individual 4\n",
      "Epoch 00091: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Done! 100. Training loss: 0.0119. Validation accuracy: 0.7426\n",
      "Individual 5\n",
      "Epoch 00092: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 00098: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Done! 100. Training loss: 0.0084. Validation accuracy: 0.7200\n",
      "Individual 6\n",
      "Done! 100. Training loss: 0.0149. Validation accuracy: 0.7253\n",
      "Individual 7\n",
      "Done! 100. Training loss: 0.0135. Validation accuracy: 0.7366\n",
      "Individual 8\n",
      "Epoch 00096: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Done! 100. Training loss: 0.0101. Validation accuracy: 0.7507\n",
      "Individual 9\n",
      "Done! 100. Training loss: 0.0245. Validation accuracy: 0.7899\n",
      "Individual 10\n",
      "Done! 100. Training loss: 0.0074. Validation accuracy: 0.7382\n",
      "Generation 14\n",
      "Individual 1\n",
      "Done! 100. Training loss: 0.0234. Validation accuracy: 0.6655\n",
      "Individual 2\n",
      "Epoch 00099: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Done! 100. Training loss: 0.0167. Validation accuracy: 0.7313\n",
      "Individual 3\n",
      "Epoch 00063: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch 00090: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch 00096: reducing learning rate of group 0 to 3.1250e-06.\n",
      "Done! 100. Training loss: 0.0093. Validation accuracy: 0.7386\n",
      "Individual 4\n",
      "Epoch 00017: reducing learning rate of group 0 to 4.5000e-05.\n",
      "Epoch 00029: reducing learning rate of group 0 to 4.0500e-05.\n",
      "Epoch 00032: reducing learning rate of group 0 to 3.6450e-05.\n",
      "Epoch 00034: reducing learning rate of group 0 to 3.2805e-05.\n",
      "Epoch 00039: reducing learning rate of group 0 to 2.9525e-05.\n",
      "Epoch 00043: reducing learning rate of group 0 to 2.6572e-05.\n",
      "Epoch 00044: reducing learning rate of group 0 to 2.3915e-05.\n",
      "Epoch 00047: reducing learning rate of group 0 to 2.1523e-05.\n",
      "Epoch 00048: reducing learning rate of group 0 to 1.9371e-05.\n",
      "Epoch 00049: reducing learning rate of group 0 to 1.7434e-05.\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.5691e-05.\n",
      "Epoch 00051: reducing learning rate of group 0 to 1.4121e-05.\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.2709e-05.\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.1438e-05.\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0295e-05.\n",
      "Epoch 00057: reducing learning rate of group 0 to 9.2651e-06.\n",
      "Epoch 00058: reducing learning rate of group 0 to 8.3386e-06.\n",
      "Epoch 00060: reducing learning rate of group 0 to 7.5047e-06.\n",
      "Epoch 00061: reducing learning rate of group 0 to 6.7543e-06.\n",
      "Epoch 00063: reducing learning rate of group 0 to 6.0788e-06.\n",
      "Epoch 00064: reducing learning rate of group 0 to 5.4709e-06.\n",
      "Epoch 00065: reducing learning rate of group 0 to 4.9239e-06.\n",
      "Epoch 00066: reducing learning rate of group 0 to 4.4315e-06.\n",
      "Epoch 00067: reducing learning rate of group 0 to 3.9883e-06.\n",
      "Epoch 00068: reducing learning rate of group 0 to 3.5895e-06.\n",
      "Epoch 00069: reducing learning rate of group 0 to 3.2305e-06.\n",
      "Epoch 00070: reducing learning rate of group 0 to 2.9075e-06.\n",
      "Epoch 00071: reducing learning rate of group 0 to 2.6167e-06.\n",
      "Epoch 00072: reducing learning rate of group 0 to 2.3551e-06.\n",
      "Epoch 00073: reducing learning rate of group 0 to 2.1196e-06.\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.9076e-06.\n",
      "Epoch 00075: reducing learning rate of group 0 to 1.7168e-06.\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.5452e-06.\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.3906e-06.\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.2516e-06.\n",
      "Epoch 00079: reducing learning rate of group 0 to 1.1264e-06.\n",
      "Epoch 00080: reducing learning rate of group 0 to 1.0138e-06.\n",
      "Epoch 00081: reducing learning rate of group 0 to 9.1240e-07.\n",
      "Epoch 00082: reducing learning rate of group 0 to 8.2116e-07.\n",
      "Epoch 00083: reducing learning rate of group 0 to 7.3904e-07.\n",
      "Epoch 00084: reducing learning rate of group 0 to 6.6514e-07.\n",
      "Epoch 00085: reducing learning rate of group 0 to 5.9863e-07.\n",
      "Epoch 00086: reducing learning rate of group 0 to 5.3876e-07.\n",
      "Epoch 00087: reducing learning rate of group 0 to 4.8489e-07.\n",
      "Epoch 00088: reducing learning rate of group 0 to 4.3640e-07.\n",
      "Epoch 00089: reducing learning rate of group 0 to 3.9276e-07.\n",
      "Epoch 00090: reducing learning rate of group 0 to 3.5348e-07.\n",
      "Epoch 00091: reducing learning rate of group 0 to 3.1813e-07.\n",
      "Epoch 00092: reducing learning rate of group 0 to 2.8632e-07.\n",
      "Epoch 00093: reducing learning rate of group 0 to 2.5769e-07.\n",
      "Epoch 00094: reducing learning rate of group 0 to 2.3192e-07.\n",
      "Epoch 00095: reducing learning rate of group 0 to 2.0873e-07.\n",
      "Epoch 00096: reducing learning rate of group 0 to 1.8786e-07.\n",
      "Epoch 00097: reducing learning rate of group 0 to 1.6907e-07.\n",
      "Epoch 00098: reducing learning rate of group 0 to 1.5216e-07.\n",
      "Epoch 00099: reducing learning rate of group 0 to 1.3695e-07.\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.2325e-07.\n",
      "Done! 100. Training loss: 0.0067. Validation accuracy: 0.7273\n",
      "Individual 5\n",
      "Epoch 00068: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch 00094: reducing learning rate of group 0 to 2.5000e-06.\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.2500e-06.\n",
      "Done! 100. Training loss: 0.0156. Validation accuracy: 0.7446\n",
      "Individual 6\n",
      "Epoch 00061: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch 00076: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch 00084: reducing learning rate of group 0 to 3.1250e-06.\n",
      "Epoch 00090: reducing learning rate of group 0 to 1.5625e-06.\n",
      "Done! 100. Training loss: 0.0090. Validation accuracy: 0.7188\n",
      "Individual 7\n",
      "Epoch 00090: reducing learning rate of group 0 to 9.0000e-06.\n",
      "Done! 100. Training loss: 0.0092. Validation accuracy: 0.7204\n",
      "Individual 8\n",
      "Done! 100. Training loss: 0.0163. Validation accuracy: 0.7345\n",
      "Individual 9\n",
      "Epoch 45. Training loss: 0.0133. Validation accuracy: 0.7354\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m genetic_algorithm(\u001b[39m10\u001b[39;49m, \u001b[39m20\u001b[39;49m, \u001b[39m0.1\u001b[39;49m, \u001b[39m0.5\u001b[39;49m, \u001b[39m2\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[44], line 31\u001b[0m, in \u001b[0;36mgenetic_algorithm\u001b[0;34m(population_size, num_generations, mutation_rate, crossover_rate, num_elites)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[1;32m     30\u001b[0m     avg_epoch_loss \u001b[39m=\u001b[39m train_epoch(train_data_loader, model, loss_fn, optimizer)\n\u001b[0;32m---> 31\u001b[0m     acc \u001b[39m=\u001b[39m calc_accuracy(val_data_loader, model, target_scaler)\n\u001b[1;32m     32\u001b[0m     scheduler\u001b[39m.\u001b[39mstep(avg_epoch_loss)\n\u001b[1;32m     33\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m. Training loss: \u001b[39m\u001b[39m{\u001b[39;00mavg_epoch_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m. Validation accuracy: \u001b[39m\u001b[39m{\u001b[39;00macc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m, in \u001b[0;36mcalc_accuracy\u001b[0;34m(valloader, model, target_scaler)\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      5\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> 6\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m valloader:\n\u001b[1;32m      7\u001b[0m         inputs, labels_unscaled \u001b[39m=\u001b[39m data\n\u001b[1;32m      8\u001b[0m         labels_unscaled \u001b[39m=\u001b[39m labels_unscaled\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/studia/SSNE/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 630\u001b[0m     \u001b[39mwith\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49mrecord_function(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_profile_name):\n\u001b[1;32m    631\u001b[0m         \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sampler_iter \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m:\n\u001b[1;32m    632\u001b[0m             \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;49;00m\n\u001b[1;32m    633\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;49;00m\n",
      "File \u001b[0;32m~/studia/SSNE/venv/lib/python3.11/site-packages/torch/autograd/profiler.py:492\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__enter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 492\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecord \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49m_record_function_enter_new(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs)\n\u001b[1;32m    493\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/studia/SSNE/venv/lib/python3.11/site-packages/torch/_ops.py:502\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    498\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "genetic_algorithm(10, 20, 0.1, 0.5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same but without the target variable\n",
    "# df_test = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess the test data\n",
    "# # One-hot encode the SubwayStation column\n",
    "# df_test = pd.get_dummies(df_test, columns=['SubwayStation'])\n",
    "# df_test = df_test.drop('SubwayStation_no_subway_nearby', axis=1)\n",
    "\n",
    "# # Map the TimeToSubway column to numerical values\n",
    "# df_test['TimeToSubway'] = df_test['TimeToSubway'].map(time_to_subway_map)\n",
    "\n",
    "# # Map the HallwayType column to numerical values\n",
    "# df_test['HallwayType'] = df_test['HallwayType'].map(hallway_type_map)\n",
    "\n",
    "# # Mapping for HeatingType column\n",
    "# df_test['HeatingType'] = df_test['HeatingType'].map(heating_type_mapping)\n",
    "\n",
    "# # Mapping for AptManageType column\n",
    "# df_test['AptManageType'] = df_test['AptManageType'].map(apt_manage_type_mapping)\n",
    "\n",
    "# # Mapping for TimeToBusStop column\n",
    "# df_test['TimeToBusStop'] = df_test['TimeToBusStop'].map(time_to_bus_stop_mapping)\n",
    "\n",
    "# # Scale the numerical features using MinMaxScaler\n",
    "# num_cols = ['YearBuilt', 'Size(sqf)', 'Floor', 'HallwayType', 'HeatingType', 'AptManageType', 'N_Parkinglot(Ground)', 'N_Parkinglot(Basement)', 'TimeToBusStop', 'N_manager', 'N_elevators', 'N_FacilitiesInApt', 'N_FacilitiesNearBy(Total)', 'N_SchoolNearBy(Total)']\n",
    "# scaler = MinMaxScaler()\n",
    "# df_test[num_cols] = scaler.fit_transform(df_test[num_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = data.TensorDataset(torch.tensor(df_test.values, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_loader = torch.utils.data.DataLoader(\n",
    "#     test_dataset,\n",
    "#     batch_size=32,\n",
    "#     shuffle=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(dataloader, model):\n",
    "#     model.eval()\n",
    "#     predictions = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch, (X,) in enumerate(dataloader):\n",
    "#             pred = model(X)\n",
    "#             pred_unscaled = target_scaler.inverse_transform(pred)\n",
    "#             pred = pred_unscaled.reshape(-1)            \n",
    "#             predictions.extend(pred.tolist())\n",
    "#     return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = predict(test_data_loader, finalModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = [0 if x <= 100000 else 1 if x <= 350000 else 2 for x in predictions]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07af8c832147382d8fb5c6ee002f61af6df906eee6315bfaf0e03fc46c78d60b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
